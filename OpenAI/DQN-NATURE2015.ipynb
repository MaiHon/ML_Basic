{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN-NATURE2015.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xhubJP6L4VEh","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import random as rd\n","import gym\n","from collections import deque"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdWlAzdm8oBC","colab_type":"code","colab":{}},"source":["env = gym.make('CartPole-v0')\n","input_size = env.observation_space.shape[0]\n","output_size = env.action_space.n\n","env._max_episode_step = 10001\n","\n","DIS = 0.99\n","MAX_EPISODE = 5000\n","REPLAY_MEMORY = 50000\n","BATCH_SIZE = 64\n","TARGET_UPDATE_FREQUENCY = 5\n","\n","# minimum epsilon for epsilon greedy\n","MIN_E = 0.0\n","# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n","EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3EneCgzOUUy","colab_type":"code","colab":{}},"source":["def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n","    \"\"\"Return an linearly annealed epsilon\n","    Epsilon will decrease over time until it reaches `target_episode`\n","         (epsilon)\n","             |\n","    max_e ---|\\\n","             | \\\n","             |  \\\n","             |   \\\n","    min_e ---|____\\_______________(episode)\n","                  |\n","                 target_episode\n","     slope = (min_e - max_e) / (target_episode)\n","     intercept = max_e\n","     e = slope * episode + intercept\n","    Args:\n","        episode (int): Current episode\n","        min_e (float): Minimum epsilon\n","        max_e (float): Maximum epsilon\n","        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n","    Returns:\n","        float: epsilon between `min_e` and `max_e`\n","    \"\"\"\n","\n","    slope = (min_e - max_e) / (target_episode)\n","    intercept = max_e\n","\n","    return max(min_e, slope * episode + intercept)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXpcldOH4Y1C","colab_type":"code","colab":{}},"source":["class DQN:\n","#   def __init__(self, sess, input_size, output_size, name=\"main\"):\n","#     self.sess = sess\n","#     self.input_size = input_size\n","#     self.output_size = output_size\n","#     self.net_name = name\n","    \n","#     self._build_network()\n","  \n","#   def _build_network(self, h_size=16, l_rate=1e-3):\n","#     with tf.device('/device:GPU:0'):\n","#       with tf.variable_scope(self.net_name):\n","#         self._x = tf.placeholder(tf.float32, shape=[None, self.input_size], name=\"input_x\")\n","#         self._keep_prob = tf.placeholder(tf.float32, name=\"prob\")\n","        \n","#         W1 = tf.get_variable('W1', shape=[self.input_size, h_size],\n","#                               initializer = tf.contrib.layers.xavier_initializer())\n","#         L1 = tf.nn.relu(tf.matmul(self._x, W1))\n","#         L1 = tf.nn.dropout(L1, self._keep_prob)\n","        \n","#         W2 = tf.get_variable('W2', shape=[h_size, h_size],\n","#                               initializer = tf.contrib.layers.xavier_initializer())\n","#         L2 = tf.nn.relu(tf.matmul(L1, W2))\n","#         L2 = tf.nn.dropout(L2, self._keep_prob)\n","        \n","        \n","#         W3 = tf.get_variable('W3', shape=[h_size, self.output_size],\n","#                               initializer = tf.contrib.layers.xavier_initializer())\n","#         self._Qpred = tf.matmul(L2, W3)\n","      \n","#       # Need to define the parts of the network needed for learning a\n","#       # Policy\n","#       self._y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n","      \n","#       # Loss\n","#       self._loss = tf.reduce_mean(tf.square(self._y - self._Qpred))\n","#       # Training\n","#       self._train = tf.train.AdamOptimizer(l_rate, epsilon=1e-5).minimize(self._loss)\n","      \n","#   def predict(self, state):\n","#     x = np.reshape(state, [-1, self.input_size])\n","#     return self.sess.run(self._Qpred, feed_dict = {self._x : x, self._keep_prob:1.0})\n","  \n","#   def update(self, x_stack, y_stack):\n","#     return self.sess.run([self._loss, self._train],\n","#                          feed_dict={self._x: x_stack, self._y: y_stack, self._keep_prob:0.7})\n","  def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n","    \"\"\"DQN Agent can\n","    1) Build network\n","    2) Predict Q_value given state\n","    3) Train parameters\n","    Args:\n","        session (tf.Session): Tensorflow session\n","        input_size (int): Input dimension\n","        output_size (int): Number of discrete actions\n","        name (str, optional): TF Graph will be built under this name scope\n","    \"\"\"\n","    self.session = session\n","    self.input_size = input_size\n","    self.output_size = output_size\n","    self.net_name = name\n","\n","    self._build_network()\n","\n","  def _build_network(self, h_size=16, l_rate=0.001) -> None:\n","    \"\"\"DQN Network architecture (simple MLP)\n","    Args:\n","        h_size (int, optional): Hidden layer dimension\n","        l_rate (float, optional): Learning rate\n","    \"\"\"\n","    with tf.device('/device:GPU:0'):\n","      with tf.variable_scope(self.net_name):\n","        self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n","#         self._prob = tf.placeholder(tf.float32, name=\"prob\")\n","        net = self._X\n","\n","        net = tf.layers.dense(net, h_size, activation=tf.nn.relu, bias_initializer=tf.contrib.layers.xavier_initializer())\n","#         net = tf.nn.dropout(net, self._prob)\n","\n","        net = tf.layers.dense(net, h_size, activation=tf.nn.relu, bias_initializer=tf.contrib.layers.xavier_initializer())\n","        net = tf.layers.dense(net, self.output_size)\n","        self._Qpred = net\n","\n","        self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n","        self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n","\n","        optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n","        self._train = optimizer.minimize(self._loss)\n","\n","  def predict(self, state: np.ndarray) -> np.ndarray:\n","      \"\"\"Returns Q(s, a)\n","      Args:\n","          state (np.ndarray): State array, shape (n, input_dim)\n","      Returns:\n","          np.ndarray: Q value array, shape (n, output_dim)\n","      \"\"\"\n","      x = np.reshape(state, [-1, self.input_size])\n","      return self.session.run(self._Qpred, feed_dict={self._X: x})\n","\n","  def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n","      \"\"\"Performs updates on given X and y and returns a result\n","      Args:\n","          x_stack (np.ndarray): State array, shape (n, input_dim)\n","          y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n","      Returns:\n","          list: First element is loss, second element is a result from train step\n","      \"\"\"\n","      feed = {\n","          self._X: x_stack,\n","          self._Y: y_stack\n","      }\n","      return self.session.run([self._loss, self._train], feed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xir5wXdz5MSt","colab_type":"code","colab":{}},"source":["def get_copy_var_ops(*, dest=\"target\", src=\"main\"):\n","  op_holder = []\n","  \n","  src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src)\n","  dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest)\n","  \n","  for src_var, dest_var in zip(src_vars, dest_vars):\n","    op_holder.append((dest_var.assign(src_var.value())))\n","  \n","  return op_holder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOeqGmdM6b2L","colab_type":"code","colab":{}},"source":["def replay_train(mainDQN, targetDQN, train_batch):\n","#   x_stack = np.empty(0).reshape(0, input_size)\n","#   y_stack = np.empty(0).reshape(0, output_size)\n","  \n","#   for state, action, reward, next_state, done in train_batch:\n","#     Q = mainDQN.predict(state)\n","#     nQ = mainDQN.predict(next_state)\n","#     tQ = targetDQN.predict(next_state)\n","    \n","#     if done:\n","#       Q[-1, action] = reward\n","#     else:\n","#       Q[-1, action] = reward + dis * tQ[-1, np.argmax(nQ)]\n","      \n","#     y_stack = np.vstack([y_stack, Q])\n","#     x_stack = np.vstack([x_stack, state])\n","\n","  states = np.vstack([x[0] for x in train_batch])\n","  actions = np.array([x[1] for x in train_batch])\n","  rewards = np.array([x[2] for x in train_batch])\n","  next_states = np.vstack([x[3] for x in train_batch])\n","  done = np.array([x[4] for x in train_batch])\n","\n","  X = states\n","\n","  Q_target = rewards + DIS * np.max(targetDQN.predict(next_states), axis=1) * ~done\n","\n","  y = mainDQN.predict(states)\n","  y[np.arange(len(X)), actions] = Q_target\n","\n","#   Train our network using target and predicted Q values on each episode\n","  return mainDQN.update(X, y)\n","\n","#   return mainDQN.update(x_stack, y_stack)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1J2bs6kJ7Xb6","colab_type":"code","colab":{}},"source":["def bot_play(mainDQN):\n","  s = env.reset()\n","  reward_sum = 0\n","  \n","  while True:\n","#     env.render()\n","    a = np.argmax(mainDQN.predict(s))\n","    s, reward, done, _ = env.step(a)\n","    reward_sum += reward\n","    \n","    if done:\n","      print(\"Total score: {}\".format(reward_sum))\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WIQ8zi_4exv","colab_type":"code","colab":{}},"source":["def main():\n","  # Store the previous observations in replay memory\n","  replay_buffer = deque(maxlen=REPLAY_MEMORY)\n","  last_100_game_reward = deque(maxlen=100)\n","  \n","  with tf.Session() as sess:\n","    main   = DQN(sess, input_size, output_size, \"main\")\n","    target = DQN(sess, input_size, output_size, \"target\")\n","    tf.global_variables_initializer().run()\n","    \n","    copy_ops = get_copy_var_ops(dest=\"target\", src=\"main\")\n","    sess.run(copy_ops)\n","    \n","    for episode in range(MAX_EPISODE):\n","      e = 1. / ((episode + 10) + 1)\n","      done = False\n","      step_cnt = 0\n","      state = env.reset()\n","      \n","      while not done:\n","        if np.random.rand(1) < e:\n","          action = env.action_space.sample()\n","        else:\n","          # Choose an action by greedily from the Q-net\n","          action = np.argmax(main.predict(state))\n","        \n","        # Get new state and reward from environment\n","        next_state, reward, done, _ = env.step(action)\n","        if done:\n","          reward = -5\n","        \n","        # Save the experience to the buffer\n","        replay_buffer.append(\n","               (state, action, reward, next_state, done)\n","        )\n","        \n","        \n","        if len(replay_buffer) > BATCH_SIZE:\n","          minibatch = rd.sample(replay_buffer, BATCH_SIZE)\n","          loss, _ = replay_train(main, target, minibatch)\n","          \n","        if step_cnt % TARGET_UPDATE_FREQUENCY == 0:\n","          sess.run(copy_ops)\n","        \n","        state = next_state\n","        step_cnt += 1\n","        if step_cnt > 10000:\n","          break\n","            \n","      print(\"Episode: {}    step: {}\".format(episode, step_cnt))\n","      last_100_game_reward.append(step_cnt)\n","      if len(last_100_game_reward) == last_100_game_reward.maxlen:\n","        avg_reward = np.mean(last_100_game_reward)\n","\n","        if avg_reward > 199:\n","            print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n","            break\n","    bot_play(main)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZ9-ulFK88BI","colab_type":"code","outputId":"a984e63b-d1d2-4a40-9875-fe46151a8ce3","executionInfo":{"status":"ok","timestamp":1558759504268,"user_tz":-540,"elapsed":216013,"user":{"displayName":"Hong Hong","photoUrl":"","userId":"17352177631675721232"}},"colab":{"base_uri":"https://localhost:8080/","height":8894}},"source":["if __name__ == \"__main__\":\n","  main()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-4-b175d3a94d20>:77: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Episode: 0    step: 10\n","Episode: 1    step: 8\n","Episode: 2    step: 10\n","Episode: 3    step: 9\n","Episode: 4    step: 10\n","Episode: 5    step: 10\n","Episode: 6    step: 11\n","Episode: 7    step: 9\n","Episode: 8    step: 12\n","Episode: 9    step: 9\n","Episode: 10    step: 9\n","Episode: 11    step: 9\n","Episode: 12    step: 9\n","Episode: 13    step: 9\n","Episode: 14    step: 10\n","Episode: 15    step: 9\n","Episode: 16    step: 10\n","Episode: 17    step: 9\n","Episode: 18    step: 10\n","Episode: 19    step: 8\n","Episode: 20    step: 10\n","Episode: 21    step: 9\n","Episode: 22    step: 10\n","Episode: 23    step: 10\n","Episode: 24    step: 9\n","Episode: 25    step: 10\n","Episode: 26    step: 9\n","Episode: 27    step: 10\n","Episode: 28    step: 8\n","Episode: 29    step: 8\n","Episode: 30    step: 9\n","Episode: 31    step: 8\n","Episode: 32    step: 8\n","Episode: 33    step: 9\n","Episode: 34    step: 9\n","Episode: 35    step: 9\n","Episode: 36    step: 11\n","Episode: 37    step: 8\n","Episode: 38    step: 9\n","Episode: 39    step: 10\n","Episode: 40    step: 10\n","Episode: 41    step: 11\n","Episode: 42    step: 10\n","Episode: 43    step: 10\n","Episode: 44    step: 11\n","Episode: 45    step: 9\n","Episode: 46    step: 8\n","Episode: 47    step: 11\n","Episode: 48    step: 11\n","Episode: 49    step: 9\n","Episode: 50    step: 9\n","Episode: 51    step: 8\n","Episode: 52    step: 10\n","Episode: 53    step: 9\n","Episode: 54    step: 11\n","Episode: 55    step: 9\n","Episode: 56    step: 10\n","Episode: 57    step: 10\n","Episode: 58    step: 10\n","Episode: 59    step: 10\n","Episode: 60    step: 10\n","Episode: 61    step: 10\n","Episode: 62    step: 8\n","Episode: 63    step: 9\n","Episode: 64    step: 10\n","Episode: 65    step: 9\n","Episode: 66    step: 10\n","Episode: 67    step: 9\n","Episode: 68    step: 9\n","Episode: 69    step: 11\n","Episode: 70    step: 10\n","Episode: 71    step: 9\n","Episode: 72    step: 9\n","Episode: 73    step: 10\n","Episode: 74    step: 10\n","Episode: 75    step: 10\n","Episode: 76    step: 9\n","Episode: 77    step: 10\n","Episode: 78    step: 9\n","Episode: 79    step: 9\n","Episode: 80    step: 10\n","Episode: 81    step: 9\n","Episode: 82    step: 9\n","Episode: 83    step: 13\n","Episode: 84    step: 9\n","Episode: 85    step: 9\n","Episode: 86    step: 10\n","Episode: 87    step: 10\n","Episode: 88    step: 9\n","Episode: 89    step: 9\n","Episode: 90    step: 11\n","Episode: 91    step: 10\n","Episode: 92    step: 9\n","Episode: 93    step: 9\n","Episode: 94    step: 10\n","Episode: 95    step: 9\n","Episode: 96    step: 10\n","Episode: 97    step: 11\n","Episode: 98    step: 8\n","Episode: 99    step: 9\n","Episode: 100    step: 11\n","Episode: 101    step: 11\n","Episode: 102    step: 9\n","Episode: 103    step: 10\n","Episode: 104    step: 10\n","Episode: 105    step: 11\n","Episode: 106    step: 10\n","Episode: 107    step: 11\n","Episode: 108    step: 9\n","Episode: 109    step: 11\n","Episode: 110    step: 10\n","Episode: 111    step: 11\n","Episode: 112    step: 10\n","Episode: 113    step: 9\n","Episode: 114    step: 9\n","Episode: 115    step: 13\n","Episode: 116    step: 9\n","Episode: 117    step: 11\n","Episode: 118    step: 9\n","Episode: 119    step: 9\n","Episode: 120    step: 10\n","Episode: 121    step: 11\n","Episode: 122    step: 10\n","Episode: 123    step: 11\n","Episode: 124    step: 12\n","Episode: 125    step: 12\n","Episode: 126    step: 10\n","Episode: 127    step: 13\n","Episode: 128    step: 10\n","Episode: 129    step: 14\n","Episode: 130    step: 10\n","Episode: 131    step: 12\n","Episode: 132    step: 15\n","Episode: 133    step: 10\n","Episode: 134    step: 14\n","Episode: 135    step: 9\n","Episode: 136    step: 10\n","Episode: 137    step: 13\n","Episode: 138    step: 9\n","Episode: 139    step: 9\n","Episode: 140    step: 17\n","Episode: 141    step: 12\n","Episode: 142    step: 11\n","Episode: 143    step: 11\n","Episode: 144    step: 15\n","Episode: 145    step: 15\n","Episode: 146    step: 15\n","Episode: 147    step: 23\n","Episode: 148    step: 26\n","Episode: 149    step: 21\n","Episode: 150    step: 20\n","Episode: 151    step: 21\n","Episode: 152    step: 21\n","Episode: 153    step: 21\n","Episode: 154    step: 22\n","Episode: 155    step: 19\n","Episode: 156    step: 23\n","Episode: 157    step: 26\n","Episode: 158    step: 27\n","Episode: 159    step: 43\n","Episode: 160    step: 22\n","Episode: 161    step: 41\n","Episode: 162    step: 33\n","Episode: 163    step: 40\n","Episode: 164    step: 42\n","Episode: 165    step: 43\n","Episode: 166    step: 58\n","Episode: 167    step: 53\n","Episode: 168    step: 56\n","Episode: 169    step: 80\n","Episode: 170    step: 94\n","Episode: 171    step: 89\n","Episode: 172    step: 85\n","Episode: 173    step: 110\n","Episode: 174    step: 128\n","Episode: 175    step: 129\n","Episode: 176    step: 106\n","Episode: 177    step: 119\n","Episode: 178    step: 50\n","Episode: 179    step: 78\n","Episode: 180    step: 122\n","Episode: 181    step: 76\n","Episode: 182    step: 89\n","Episode: 183    step: 69\n","Episode: 184    step: 200\n","Episode: 185    step: 87\n","Episode: 186    step: 51\n","Episode: 187    step: 104\n","Episode: 188    step: 85\n","Episode: 189    step: 48\n","Episode: 190    step: 52\n","Episode: 191    step: 44\n","Episode: 192    step: 29\n","Episode: 193    step: 43\n","Episode: 194    step: 59\n","Episode: 195    step: 114\n","Episode: 196    step: 92\n","Episode: 197    step: 111\n","Episode: 198    step: 132\n","Episode: 199    step: 100\n","Episode: 200    step: 151\n","Episode: 201    step: 187\n","Episode: 202    step: 143\n","Episode: 203    step: 200\n","Episode: 204    step: 200\n","Episode: 205    step: 45\n","Episode: 206    step: 84\n","Episode: 207    step: 60\n","Episode: 208    step: 68\n","Episode: 209    step: 68\n","Episode: 210    step: 37\n","Episode: 211    step: 51\n","Episode: 212    step: 59\n","Episode: 213    step: 51\n","Episode: 214    step: 37\n","Episode: 215    step: 200\n","Episode: 216    step: 101\n","Episode: 217    step: 54\n","Episode: 218    step: 40\n","Episode: 219    step: 53\n","Episode: 220    step: 83\n","Episode: 221    step: 200\n","Episode: 222    step: 50\n","Episode: 223    step: 200\n","Episode: 224    step: 200\n","Episode: 225    step: 200\n","Episode: 226    step: 200\n","Episode: 227    step: 200\n","Episode: 228    step: 187\n","Episode: 229    step: 97\n","Episode: 230    step: 200\n","Episode: 231    step: 200\n","Episode: 232    step: 200\n","Episode: 233    step: 200\n","Episode: 234    step: 107\n","Episode: 235    step: 200\n","Episode: 236    step: 200\n","Episode: 237    step: 200\n","Episode: 238    step: 200\n","Episode: 239    step: 200\n","Episode: 240    step: 200\n","Episode: 241    step: 200\n","Episode: 242    step: 200\n","Episode: 243    step: 200\n","Episode: 244    step: 200\n","Episode: 245    step: 200\n","Episode: 246    step: 200\n","Episode: 247    step: 200\n","Episode: 248    step: 200\n","Episode: 249    step: 200\n","Episode: 250    step: 200\n","Episode: 251    step: 200\n","Episode: 252    step: 200\n","Episode: 253    step: 200\n","Episode: 254    step: 200\n","Episode: 255    step: 200\n","Episode: 256    step: 200\n","Episode: 257    step: 200\n","Episode: 258    step: 200\n","Episode: 259    step: 200\n","Episode: 260    step: 200\n","Episode: 261    step: 200\n","Episode: 262    step: 200\n","Episode: 263    step: 200\n","Episode: 264    step: 200\n","Episode: 265    step: 200\n","Episode: 266    step: 200\n","Episode: 267    step: 186\n","Episode: 268    step: 200\n","Episode: 269    step: 200\n","Episode: 270    step: 200\n","Episode: 271    step: 193\n","Episode: 272    step: 200\n","Episode: 273    step: 200\n","Episode: 274    step: 200\n","Episode: 275    step: 200\n","Episode: 276    step: 200\n","Episode: 277    step: 200\n","Episode: 278    step: 200\n","Episode: 279    step: 200\n","Episode: 280    step: 200\n","Episode: 281    step: 200\n","Episode: 282    step: 200\n","Episode: 283    step: 200\n","Episode: 284    step: 200\n","Episode: 285    step: 200\n","Episode: 286    step: 200\n","Episode: 287    step: 200\n","Episode: 288    step: 200\n","Episode: 289    step: 200\n","Episode: 290    step: 200\n","Episode: 291    step: 200\n","Episode: 292    step: 200\n","Episode: 293    step: 200\n","Episode: 294    step: 200\n","Episode: 295    step: 200\n","Episode: 296    step: 200\n","Episode: 297    step: 200\n","Episode: 298    step: 200\n","Episode: 299    step: 200\n","Episode: 300    step: 200\n","Episode: 301    step: 200\n","Episode: 302    step: 200\n","Episode: 303    step: 200\n","Episode: 304    step: 200\n","Episode: 305    step: 200\n","Episode: 306    step: 200\n","Episode: 307    step: 200\n","Episode: 308    step: 200\n","Episode: 309    step: 200\n","Episode: 310    step: 200\n","Episode: 311    step: 200\n","Episode: 312    step: 200\n","Episode: 313    step: 200\n","Episode: 314    step: 200\n","Episode: 315    step: 166\n","Episode: 316    step: 139\n","Episode: 317    step: 11\n","Episode: 318    step: 130\n","Episode: 319    step: 11\n","Episode: 320    step: 120\n","Episode: 321    step: 10\n","Episode: 322    step: 8\n","Episode: 323    step: 10\n","Episode: 324    step: 10\n","Episode: 325    step: 10\n","Episode: 326    step: 10\n","Episode: 327    step: 9\n","Episode: 328    step: 8\n","Episode: 329    step: 10\n","Episode: 330    step: 9\n","Episode: 331    step: 9\n","Episode: 332    step: 9\n","Episode: 333    step: 10\n","Episode: 334    step: 9\n","Episode: 335    step: 9\n","Episode: 336    step: 8\n","Episode: 337    step: 10\n","Episode: 338    step: 9\n","Episode: 339    step: 8\n","Episode: 340    step: 10\n","Episode: 341    step: 135\n","Episode: 342    step: 186\n","Episode: 343    step: 200\n","Episode: 344    step: 121\n","Episode: 345    step: 126\n","Episode: 346    step: 136\n","Episode: 347    step: 173\n","Episode: 348    step: 166\n","Episode: 349    step: 200\n","Episode: 350    step: 200\n","Episode: 351    step: 200\n","Episode: 352    step: 200\n","Episode: 353    step: 200\n","Episode: 354    step: 200\n","Episode: 355    step: 200\n","Episode: 356    step: 200\n","Episode: 357    step: 200\n","Episode: 358    step: 200\n","Episode: 359    step: 186\n","Episode: 360    step: 152\n","Episode: 361    step: 159\n","Episode: 362    step: 163\n","Episode: 363    step: 169\n","Episode: 364    step: 175\n","Episode: 365    step: 163\n","Episode: 366    step: 196\n","Episode: 367    step: 195\n","Episode: 368    step: 184\n","Episode: 369    step: 200\n","Episode: 370    step: 182\n","Episode: 371    step: 166\n","Episode: 372    step: 168\n","Episode: 373    step: 181\n","Episode: 374    step: 190\n","Episode: 375    step: 180\n","Episode: 376    step: 165\n","Episode: 377    step: 173\n","Episode: 378    step: 163\n","Episode: 379    step: 200\n","Episode: 380    step: 155\n","Episode: 381    step: 130\n","Episode: 382    step: 162\n","Episode: 383    step: 158\n","Episode: 384    step: 136\n","Episode: 385    step: 158\n","Episode: 386    step: 172\n","Episode: 387    step: 200\n","Episode: 388    step: 172\n","Episode: 389    step: 150\n","Episode: 390    step: 166\n","Episode: 391    step: 162\n","Episode: 392    step: 200\n","Episode: 393    step: 166\n","Episode: 394    step: 164\n","Episode: 395    step: 174\n","Episode: 396    step: 148\n","Episode: 397    step: 164\n","Episode: 398    step: 182\n","Episode: 399    step: 176\n","Episode: 400    step: 198\n","Episode: 401    step: 188\n","Episode: 402    step: 183\n","Episode: 403    step: 185\n","Episode: 404    step: 197\n","Episode: 405    step: 194\n","Episode: 406    step: 198\n","Episode: 407    step: 200\n","Episode: 408    step: 200\n","Episode: 409    step: 200\n","Episode: 410    step: 200\n","Episode: 411    step: 200\n","Episode: 412    step: 200\n","Episode: 413    step: 200\n","Episode: 414    step: 200\n","Episode: 415    step: 200\n","Episode: 416    step: 142\n","Episode: 417    step: 200\n","Episode: 418    step: 200\n","Episode: 419    step: 200\n","Episode: 420    step: 200\n","Episode: 421    step: 200\n","Episode: 422    step: 200\n","Episode: 423    step: 200\n","Episode: 424    step: 200\n","Episode: 425    step: 200\n","Episode: 426    step: 200\n","Episode: 427    step: 200\n","Episode: 428    step: 200\n","Episode: 429    step: 200\n","Episode: 430    step: 200\n","Episode: 431    step: 200\n","Episode: 432    step: 200\n","Episode: 433    step: 200\n","Episode: 434    step: 200\n","Episode: 435    step: 200\n","Episode: 436    step: 200\n","Episode: 437    step: 200\n","Episode: 438    step: 200\n","Episode: 439    step: 200\n","Episode: 440    step: 200\n","Episode: 441    step: 200\n","Episode: 442    step: 200\n","Episode: 443    step: 200\n","Episode: 444    step: 200\n","Episode: 445    step: 200\n","Episode: 446    step: 200\n","Episode: 447    step: 200\n","Episode: 448    step: 200\n","Episode: 449    step: 200\n","Episode: 450    step: 200\n","Episode: 451    step: 200\n","Episode: 452    step: 200\n","Episode: 453    step: 200\n","Episode: 454    step: 200\n","Episode: 455    step: 200\n","Episode: 456    step: 200\n","Episode: 457    step: 200\n","Episode: 458    step: 200\n","Episode: 459    step: 200\n","Episode: 460    step: 200\n","Episode: 461    step: 200\n","Episode: 462    step: 200\n","Episode: 463    step: 200\n","Episode: 464    step: 200\n","Episode: 465    step: 200\n","Episode: 466    step: 200\n","Episode: 467    step: 200\n","Episode: 468    step: 200\n","Episode: 469    step: 200\n","Episode: 470    step: 200\n","Episode: 471    step: 200\n","Episode: 472    step: 200\n","Episode: 473    step: 200\n","Episode: 474    step: 200\n","Episode: 475    step: 200\n","Episode: 476    step: 200\n","Episode: 477    step: 200\n","Episode: 478    step: 200\n","Episode: 479    step: 200\n","Episode: 480    step: 200\n","Episode: 481    step: 200\n","Episode: 482    step: 200\n","Episode: 483    step: 200\n","Episode: 484    step: 200\n","Episode: 485    step: 200\n","Episode: 486    step: 200\n","Episode: 487    step: 200\n","Episode: 488    step: 200\n","Episode: 489    step: 200\n","Episode: 490    step: 200\n","Episode: 491    step: 200\n","Episode: 492    step: 200\n","Episode: 493    step: 200\n","Episode: 494    step: 200\n","Episode: 495    step: 200\n","Episode: 496    step: 200\n","Episode: 497    step: 200\n","Episode: 498    step: 200\n","Episode: 499    step: 200\n","Episode: 500    step: 200\n","Episode: 501    step: 200\n","Episode: 502    step: 200\n","Game Cleared in 502 episodes with avg reward 199.16\n","Total score: 200.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UL6kJRMDN0CM","colab_type":"code","colab":{}},"source":["def main():\n","  max_episodes = 5000\n","  \n","  # Store the previous observations in replay memory\n","  replay_buffer = deque(maxlen=REPLAY_MEMORY)\n","  last_100_game_reward = deque(maxlen=100)\n","  \n","  with tf.Session() as sess:\n","    main   = DQN(sess, input_size, output_size, \"main2\")\n","    target = DQN(sess, input_size, output_size, \"target2\")\n","    tf.global_variables_initializer().run()\n","    \n","    copy_ops = get_copy_var_ops(dest=\"target2\", src=\"main2\")\n","    sess.run(copy_ops)\n","    \n","    for episode in range(max_episodes):\n","      e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n","      done = False\n","      step_cnt = 0\n","      state = env.reset()\n","      \n","      while not done:\n","        if np.random.rand(1) < e:\n","          action = env.action_space.sample()\n","        else:\n","          # Choose an action by greedily from the Q-net\n","          action = np.argmax(main.predict(state))\n","        \n","        # Get new state and reward from environment\n","        next_state, reward, done, _ = env.step(action)\n","        if done:\n","          reward = -5\n","        \n","        # Save the experience to the buffer\n","        replay_buffer.append(\n","               (state, action, reward, next_state, done)\n","        )\n","        \n","        \n","        if len(replay_buffer) > BATCH_SIZE:\n","          minibatch = rd.sample(replay_buffer, BATCH_SIZE)\n","          loss, _ = replay_train(main, target, minibatch)\n","          \n","        if step_cnt % TARGET_UPDATE_FREQUENCY == 0:\n","          sess.run(copy_ops)\n","        \n","        state = next_state\n","        step_cnt += 1\n","        if step_cnt > 10000:\n","          break\n","            \n","      print(\"Episode: {}    step: {}\".format(episode, step_cnt))\n","      last_100_game_reward.append(step_cnt)\n","      if len(last_100_game_reward) == last_100_game_reward.maxlen:\n","        avg_reward = np.mean(last_100_game_reward)\n","\n","        if avg_reward > 199:\n","            print(f\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n","            break\n","    bot_play(main)"],"execution_count":0,"outputs":[]}]}