{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab11_CNN_CIFAR10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hCs7eSkzSDBX","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5iC0MsLgBHf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c735f487-657b-4cff-c6ba-c27ceef30342","executionInfo":{"status":"ok","timestamp":1570590813837,"user_tz":-540,"elapsed":1412,"user":{"displayName":"Hong Hong","photoUrl":"","userId":"17352177631675721232"}}},"source":["tf.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0-rc2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"GiyJeIDkSIGW","colab_type":"code","colab":{}},"source":["# CIFAR-10 데이터를 다운로드 받기 위한 keras의 helper 함수인 load_data 함수를 임포트합니다.\n","from tensorflow.keras.datasets.cifar10 import load_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sl6SKcIySthf","colab_type":"code","colab":{}},"source":["# def next_batch(num, data, labels):\n","#   '''\n","#   `num` 개수 만큼의 랜덤한 샘플들과 레이블들을 리턴합니다.\n","#   '''\n","#   idx = np.arange(0 , len(data))\n","#   np.random.shuffle(idx)\n","#   idx = idx[:num]\n","#   data_shuffle = [data[ i] for i in idx]\n","#   labels_shuffle = [labels[ i] for i in idx]\n","#   return np.asarray(data_shuffle), np.asarray(labels_shuffle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mw_Mc4ywUG9i","colab_type":"code","outputId":"47bc6a28-153d-481b-d649-c95539c84a01","executionInfo":{"status":"ok","timestamp":1570590843347,"user_tz":-540,"elapsed":15938,"user":{"displayName":"Hong Hong","photoUrl":"","userId":"17352177631675721232"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["(x_train, y_train), (x_test, y_test) = load_data()\n","\n","# y_train = tf.squeeze(tf.one_hot(y_train, 10), axis=1)\n","# y_test = tf.squeeze(tf.one_hot(y_test, 10), axis=1)\n","n_classes = np.max(y_train) + 1\n","y_train = np.eye(n_classes)[np.squeeze(y_train)]\n","y_test = np.eye(n_classes)[np.squeeze(y_test)]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 12s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t3sQNU5TZceS","colab_type":"code","colab":{}},"source":["def standardzation(data, momentum=0.99, epsilon=1e-4):\n","  mean = np.mean(data)\n","  variance = np.var(data)\n","  \n","#   print(\"mean:\", mean)\n","#   print(\"variance:\",variance)\n","  \n","  return momentum * ((data - mean) / np.sqrt(variance**2 + 1e-6)) + epsilon\n","\n","\n","x_test = standardzation(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0sCtnMyTtbt","colab_type":"code","colab":{}},"source":["class Model_CIFAR10:\n","  def __init__(self, sess, name, decay = 0.96, learning_rate = 3e-4):\n","    self.sess = sess\n","    self.name = name\n","    self.decay = decay\n","    self.learning_rate = learning_rate\n","    self.build_net()\n","    \n","  def build_net(self):\n","    with tf.device('/device:GPU:0'):\n","      with tf.variable_scope(self.name):\n","        self.x = tf.placeholder(tf.float32, [None, 32, 32, 3], name = 'x')        \n","        self.y = tf.placeholder(tf.float32, [None, 10], name='y')\n","        self.keep_prob = tf.placeholder(tf.float32, name = 'prob')\n","\n","        # [?, 32, 32, 3] -> [?, 16, 16, 64]\n","        with tf.name_scope(\"Layer1\"):\n","          W1 = tf.get_variable('W1', shape = [3, 3, 3, 64],\n","                                            initializer = tf.keras.initializers.he_normal())\n","          L1 = tf.nn.conv2d(self.x, W1, strides=[1, 1, 1, 1], padding='SAME')\n","          L1 = tf.nn.relu(L1)\n","#           L1 = tf.math.tanh(L1)\n","          L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","          L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n","\n","\n","        with tf.name_scope(\"Layer2\"):\n","          # [?, 16, 16, 64] -> [?, 8, 8, 64]\n","          W2 = tf.get_variable('W2', shape = [3, 3, 64, 128],\n","                                            initializer=tf.keras.initializers.he_normal())\n","          L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","          L2 = tf.nn.relu(L2)\n","#           L2 = tf.math.tanh(L2)\n","          L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n","          L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n","    \n","        with tf.name_scope(\"Layer4\"):\n","          # [?, 8, 8, 64] -> [?, 4, 4, 128]\n","          W4 = tf.get_variable('W4', shape = [3, 3, 128, 128],\n","                                            initializer=tf.keras.initializers.he_normal())\n","          L4 = tf.nn.conv2d(L2, W4, strides=[1, 1, 1, 1], padding='SAME')\n","          L4 = tf.nn.relu(L4)\n","#           L4 = tf.math.tanh(L4)\n","          L4 = tf.nn.max_pool(L4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n","          L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n","          L4_flat = tf.reshape(L4, [-1, 4 * 4 * 128])\n","          \n","          \n","        with tf.name_scope(\"FC1\"):\n","          # [?, 4 * 4 * 256] -> [?, 256]\n","          W6 = tf.get_variable('W6', shape=[128 * 4 * 4, 128], \n","                                            initializer=tf.keras.initializers.he_normal())\n","          b6 = tf.get_variable('b6', shape=[128], initializer=tf.keras.initializers.zeros())\n","          FC1 = tf.keras.layers.BatchNormalization()(tf.nn.relu(tf.matmul(L4_flat, W6) + b6), training=True)\n","#           FC1 = tf.math.tanh(tf.matmul(L4_flat, W6) + b6)\n","          FC1 = tf.nn.dropout(FC1, keep_prob=self.keep_prob)\n","\n","\n","        with tf.name_scope(\"FC2\"):\n","          # [?, 128] -> [? 10]\n","          W7 = tf.get_variable('W7', shape=[128, 10], \n","                                            initializer=tf.keras.initializers.he_normal())\n","          b7 = tf.get_variable('b7', shape=[10], initializer=tf.keras.initializers.zeros())\n","          self.logits = tf.matmul(FC1, W7) + b7\n","          self.y_pred = tf.nn.softmax(self.logits)\n","                  \n","        with tf.name_scope(\"Cost\"):\n","          self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n","                logits=self.logits, labels=self.y))\n","          \n","        with tf.name_scope(\"Train\"):\n","          self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, \n","                                                                epsilon=1e-5).minimize(self.cost)\n","\n","        correct_prediction = tf.equal(tf.argmax(self.y_pred, 1), tf.argmax(self.y, 1))\n","        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  \n","  def set_learningRate(self, learning_rate):\n","    self.learning_rate = learning_rate\n","  \n","  def predict(self, x_data, keep_prob=1.0):\n","    return self.sess.run(self.y_pred, feed_dict={self.x: x_data, self.keep_prob: keep_prop})\n","\n","  def get_accuracy(self, x_data, y_data, keep_prob=1.0):\n","    return self.sess.run(self.accuracy, feed_dict={self.x: x_data, self.y: y_data, self.keep_prob:keep_prob})\n","\n","  def train(self, x_data, y_data, keep_prob=0.2):\n","    return self.sess.run([self.cost, self.optimizer],\n","                                      feed_dict = {self.x:x_data, self.y:y_data, self.keep_prob:keep_prob})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xupmy4B2bTtI","colab_type":"code","outputId":"d9d768e7-4f81-4009-d171-5635265df9ac","executionInfo":{"status":"error","timestamp":1559062668542,"user_tz":-540,"elapsed":97268,"user":{"displayName":"Hong Hong","photoUrl":"","userId":"17352177631675721232"}},"colab":{"base_uri":"https://localhost:8080/","height":633}},"source":["import random as rd\n","\n","training_epochs = 100\n","batch_size = 100\n","\n","\n","\n","with tf.Session() as sess:\n","  model = Model_CIFAR10(sess, 'M3', learning_rate=0.1)\n","  sess.run(tf.global_variables_initializer())\n","  \n","  print(\"Learning Start! with learning_rate:\", model.learning_rate)\n","  \n","  # define dataset for next batch\n","  dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  dataset = dataset.shuffle(20000)\n","  dataset = dataset.repeat(training_epochs)\n","  dataset = dataset.batch(batch_size)\n","\n","  # get iterator\n","  iterator = dataset.make_one_shot_iterator()\n","  next_batch = iterator.get_next()  \n","  global_step = 0\n","  \n","  for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(x_train.shape[0] / batch_size)\n","    \n","    decay = model.decay ** (global_step / 100000)\n","    learning_rate = model.learning_rate * decay\n","    model.set_learningRate(learning_rate)\n","    noise = np.random.rand(1) * (10 ** -5)\n","    \n","    for i in range(total_batch):\n","      batch_xs, batch_ys = sess.run(next_batch)\n","      batch_xs = standardzation(batch_xs)\n","      if global_step % 5 != 0:\n","        batch_xs *= noise      \n","        \n","      c, _ = model.train(batch_xs, batch_ys)\n","      avg_cost += c / total_batch\n","      global_step += 1\n","    \n","    print(\"Epoch: {:04d}     Global Step: {:07d}    Cost: {:.9f}     Test Accuracy: {:.5f}    Learning Rate: {:.15f}\".format(epoch+1, \n","                                                                                                global_step,\n","                                                                                                avg_cost, \n","                                                                                                model.get_accuracy(x_test, y_test),\n","                                                                                                model.learning_rate))\n","  print(\"Learning Complete!!\")\n","  \n","  print(\"Accuracy:\", model.get_accuracy(x_test, y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Learning Start! with learning_rate: 0.1\n","Epoch: 0001     Global Step: 0000500    Cost: 2.501124313     Test Accuracy: 0.17750    Learning Rate: 0.100000000000000\n","Epoch: 0002     Global Step: 0001000    Cost: 2.327254827     Test Accuracy: 0.17420    Learning Rate: 0.099979591085642\n","Epoch: 0003     Global Step: 0001500    Cost: 2.527359648     Test Accuracy: 0.13990    Learning Rate: 0.099938785751790\n","Epoch: 0004     Global Step: 0002000    Cost: 2.500388251     Test Accuracy: 0.18140    Learning Rate: 0.099877608975422\n","Epoch: 0005     Global Step: 0002500    Cost: 2.460459790     Test Accuracy: 0.15440    Learning Rate: 0.099796098190152\n","Epoch: 0006     Global Step: 0003000    Cost: 2.394696096     Test Accuracy: 0.19700    Learning Rate: 0.099694303248058\n","Epoch: 0007     Global Step: 0003500    Cost: 2.370100451     Test Accuracy: 0.16870    Learning Rate: 0.099572286368867\n","Epoch: 0008     Global Step: 0004000    Cost: 2.330858020     Test Accuracy: 0.18740    Learning Rate: 0.099430122076577\n","Epoch: 0009     Global Step: 0004500    Cost: 2.329684256     Test Accuracy: 0.17920    Learning Rate: 0.099267897123603\n","Epoch: 0010     Global Step: 0005000    Cost: 2.305132141     Test Accuracy: 0.23820    Learning Rate: 0.099085710402547\n","Epoch: 0011     Global Step: 0005500    Cost: 2.272197806     Test Accuracy: 0.19670    Learning Rate: 0.098883672845735\n","Epoch: 0012     Global Step: 0006000    Cost: 2.278809403     Test Accuracy: 0.23990    Learning Rate: 0.098661907312650\n","Epoch: 0013     Global Step: 0006500    Cost: 2.236408971     Test Accuracy: 0.24140    Learning Rate: 0.098420548465418\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-c93bb0a7f8f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbatch_xs\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-d5b47adb8712>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_data, y_data, keep_prob)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     return self.sess.run([self.cost, self.optimizer],\n\u001b[0;32m---> 89\u001b[0;31m                                       feed_dict = {self.x:x_data, self.y:y_data, self.keep_prob:keep_prob})\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"TsyHZw_ijCaa","colab_type":"code","colab":{}},"source":["# Set parameters\n","LEARN_RATE = 1e-4\n","TRAIN_EPOCH = 15\n","BATCH_SIZE = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YT_LvNFvdCn2","colab_type":"code","colab":{}},"source":["class Model_CIFAR10(tf.keras.Model):\n","  def __init__(self):\n","    super(Model_CIFAR10, self).__init__()\n","    self.conv1 = keras.layers.Conv2D(32, kernel_size=3, padding='same', activation=tf.nn.relu)\n","    self.pool1 = keras.layers.MaxPooling2D(padding='same')\n","    self.conv2 = keras.layers.Conv2D(64, kernel_size=3, padding='same', activation=tf.nn.relu)\n","    self.pool2 = keras.layers.MaxPooling2D(padding='same')\n","    self.conv3 = keras.layers.Conv2D(128, kernel_size=3, padding='same', activation=tf.nn.relu)\n","    self.pool3 = keras.layers.MaxPooling2D(padding='same')\n","    \n","    self.pool3_flat = keras.layers.Flatten()\n","    self.dense4 = keras.layers.Dense(256, tf.nn.relu)\n","    self.drop4 = keras.layers.Dropout(0.4)\n","    self.dense5 = keras.layers.Dense(10, tf.nn.softmax)\n","  def call(self, inputs, trainin=False):\n","    net = self.conv1(inputs)\n","    net = self.pool1(net)\n","    net = self.conv2(net)\n","    net = self.pool2(net)\n","    net = self.conv3(net)\n","    net = self.pool3(net)\n","    net = self.pool3_flat(net)\n","    net = self.dense4(net)\n","    net = self.drop4(net)\n","    net = self.dense5(net)\n","    \n","    return net"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsaV-6Z3i_tu","colab_type":"code","colab":{}},"source":["def loss_fn(model, images, labels):\n","  logits = model(images, training=True)\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n","                                                                   logits=logits))\n","  \n","  return loss\n","\n","def grad(model, images, labels):\n","  with tf.GradientTape() as tape:\n","    loss = loss_fn(model, images, labels)\n","    \n","  return tape.gradient(loss, model.variables)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MX9eiGZhi8Z","colab_type":"code","colab":{}},"source":["models = []\n","num_models = 3\n","\n","for m in range(num_models):\n","  models.append(Model_CIFAR10())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukM_z37GioVV","colab_type":"code","colab":{}},"source":["train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=100000).batch(BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RM1pjQGy6k99","colab_type":"code","colab":{}},"source":["|"],"execution_count":0,"outputs":[]}]}